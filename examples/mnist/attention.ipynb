{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention on MNIST (Saliency and grad-CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the mnist model and train it for 5 epochs. It should get to about ~99% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.2493 - acc: 0.9253 - val_loss: 0.0519 - val_acc: 0.9833\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0908 - acc: 0.9728 - val_loss: 0.0416 - val_acc: 0.9870\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0671 - acc: 0.9791 - val_loss: 0.0321 - val_acc: 0.9889\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0541 - acc: 0.9832 - val_loss: 0.0311 - val_acc: 0.9898\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0445 - acc: 0.9862 - val_loss: 0.0282 - val_acc: 0.9915\n",
      "Test loss: 0.028204882503575755\n",
      "Test accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 256, 256\n",
    "\n",
    "x_train = train_db[\"features\"]\n",
    "y_train = train_db[\"labels\"]\n",
    "x_test = test_db[\"features\"]\n",
    "y_test = test_db[\"labels\"]\n",
    "\n",
    "y_train = ((y_train + 1) // 2)\n",
    "y_train = np.array([y_train, 1-y_train]).transpose()\n",
    "\n",
    "y_test = ((y_test + 1) // 2)\n",
    "y_test = np.array([y_test, 1-y_test]).transpose()\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 256, 256, 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, 256, 256, 1))\n",
    "\n",
    "print(f\"Train size: {X_train.shape}\")\n",
    "print(f\"Test size: {X_test.shape}\")\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax', name='preds'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear' and compare what happens if we dont do this at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pick an input over which we want to show the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb19008b2e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_idx = 0\n",
    "indices = np.where(y_test[:, class_idx] == 1.)[0]\n",
    "\n",
    "# pick some random input from here.\n",
    "idx = indices[0]\n",
    "\n",
    "# Lets sanity check the picked image.\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "plt.imshow(x_test[idx][..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for saliency visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-faedabb421eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_saliency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Utility to search for layer index by name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vis'"
     ]
    }
   ],
   "source": [
    "from vis.visualization import visualize_saliency\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'preds')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, seed_input=x_test[idx])\n",
    "# Plot with 'jet' colormap to visualize as a heatmap.\n",
    "plt.imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To used guided saliency, we need to set `backprop_modifier='guided'`. For rectified saliency or deconv saliency, use `backprop_modifier='relu'`. Lets try these options quickly and see how they compare to vanilla saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modifier in ['guided', 'relu']:\n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=class_idx,\n",
    "                               seed_input=x_test[idx], backprop_modifier=modifier)\n",
    "    plt.figure()\n",
    "    plt.title(modifier)\n",
    "    plt.imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of them look a lot better than vanilla saliency! This in inline with observation in the paper.\n",
    "\n",
    "We can also visualize negative gradients to see the parts of the image that contribute negatively to the output by using `grad_modifier='negate'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, seed_input=x_test[idx], \n",
    "                           backprop_modifier='guided', grad_modifier='negate')\n",
    "plt.imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try all the classes and show original inputs and their heatmaps side by side. We cannot overlay the heatmap on original image since its grayscale.\n",
    "\n",
    "We will also compare the outputs of guided and rectified or deconv saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This corresponds to the Dense linear layer.\n",
    "for class_idx in np.arange(10):    \n",
    "    indices = np.where(y_test[:, class_idx] == 1.)[0]\n",
    "    idx = indices[0]\n",
    "\n",
    "    f, ax = plt.subplots(1, 4)\n",
    "    ax[0].imshow(x_test[idx][..., 0])\n",
    "    \n",
    "    for i, modifier in enumerate([None, 'guided', 'relu']):\n",
    "        grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, \n",
    "                                   seed_input=x_test[idx], backprop_modifier=modifier)\n",
    "        if modifier is None:\n",
    "            modifier = 'vanilla'\n",
    "        ax[i+1].set_title(modifier)    \n",
    "        ax[i+1].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guided saliency seems to give the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad-CAM - vanilla, guided, rectified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should contain more detail since they use `Conv` or `Pooling` features that contain more spatial detail which is lost in `Dense` layers. The only additional detail compared to saliency is the `penultimate_layer_idx`. This specifies the pre-layer whose gradients should be used. See this paper for technical details: https://arxiv.org/pdf/1610.02391v1.pdf\n",
    "\n",
    "By default, if `penultimate_layer_idx` is not defined, it searches for the nearest pre layer. For our architecture, that would be the `MaxPooling2D` layer after all the `Conv` layers. Lets look at all the visualizations like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_cam\n",
    "\n",
    "# This corresponds to the Dense linear layer.\n",
    "for class_idx in np.arange(10):    \n",
    "    indices = np.where(y_test[:, class_idx] == 1.)[0]\n",
    "    idx = indices[0]\n",
    "\n",
    "    f, ax = plt.subplots(1, 4)\n",
    "    ax[0].imshow(x_test[idx][..., 0])\n",
    "    \n",
    "    for i, modifier in enumerate([None, 'guided', 'relu']):\n",
    "        grads = visualize_cam(model, layer_idx, filter_indices=class_idx, \n",
    "                              seed_input=x_test[idx], backprop_modifier=modifier)        \n",
    "        if modifier is None:\n",
    "            modifier = 'vanilla'\n",
    "        ax[i+1].set_title(modifier)    \n",
    "        ax[i+1].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it appears that saliency is better than grad-CAM as penultimate `MaxPooling2D` layer has `(12, 12)` spatial resolution which is relatively large as compared to input of `(28, 28)`. Is is likely that the conv layer hasnt captured enough high level information and most of that is likely within `dense_4` layer. \n",
    "\n",
    "Here is the model summary for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization without swapping softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded at the beginning of the tutorial, we want to compare and see what happens if we didnt swap out softmax for linear activation. Lets try this with guided saliency which gave us the best results so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Swap linear back with softmax\n",
    "model.layers[layer_idx].activation = activations.softmax\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "for class_idx in np.arange(10):    \n",
    "    indices = np.where(y_test[:, class_idx] == 1.)[0]\n",
    "    idx = indices[0]\n",
    "    \n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, \n",
    "                               seed_input=x_test[idx], backprop_modifier='guided')\n",
    "\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(x_test[idx][..., 0])\n",
    "    ax[1].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not work as well! \n",
    "\n",
    "It does not work! The reason is that maximizing an output node can be done by minimizing other outputs. Softmax is weird that way. It is the only activation that depends on other node output(s) in the layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
